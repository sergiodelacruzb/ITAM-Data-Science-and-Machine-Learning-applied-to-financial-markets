{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia estadística\n",
    "\n",
    "## Estadísticas suficientes\n",
    "\n",
    "**Definición(Muestra)**\n",
    "\n",
    "Una muestra, $X_1,\\ldots,X_n$, es un conjunto de variables (observables) aleatorias independientes e idénticamente distribuidas.\n",
    "\n",
    "En la práctica, se supone que una muestra proviene de una población cuya distribución $f(x)$ está parametrizada por un vector de parámetros $\\mathbf{\\theta}$. Este vector $\\mathbf{\\theta}$, captura los aspectos relevantes de la distribución, por ejemplo en el caso de la distribución $N(\\mu, \\sigma^2)$, $\\mathbf{\\theta} = (\\mu, \\sigma^2)$.\n",
    "\n",
    "Desde el punto de vista de la estadística frecuentista, el vector $\\mathbf{\\theta}$ se considera desconocido **pero fijo** y el objetivo de la inferencia estadística es obtener información acerca de él, utilizando la muestra $X_1,\\ldots,X_n$.\n",
    "\n",
    "**Definición(Estadística)**\n",
    "\n",
    "Una función **observable**, $T(X_1,\\ldots,X_n)$, es llamada una estadística.\n",
    "\n",
    "El término **observable** se refiere a que $T(X_1,\\ldots,X_n)$ no puede involucrar cantidades desconocidas, por ejemplo, algún elemento del vector $\\mathbf{\\theta}$. $T(X_1,\\ldots,X_n)$ se determina únicamente a través de los valores de observados para en la muestra.\n",
    "\n",
    "**Definición(Estadística suficiente)**\n",
    "\n",
    "Una estadística, $T$, es llamada **suficiente**  para un parámetro desconocido $\\mathbf{\\theta}$ si y sólo si la distribución condicional de la muestra $\\mathbf{X} = (X_1,\\ldots,X_n)$ dado $T=t$ no involucra $\\mathbf{\\theta}$ para todo $t$ en el soporte de $T$.\n",
    "\n",
    "Así pues, una vez habiendo observado el valor de una **estadística suficiente**, la muestra $(X_1,\\ldots,X_n)$ no nos puede brindar más información acerca de $\\mathbf{\\theta}$.\n",
    "\n",
    "**Definición(Función de verosimilitud)**\n",
    "\n",
    "Una vez observando los valores $X_i = x_i$, $i=1,\\ldots,n$; la función de verosimilitud (en inglés likelihood) se define como una función de $\\mathbf{\\theta}$\n",
    "\n",
    "$$\n",
    "L(\\mathbf{\\theta}) = \\Pi_{i=1}^n f(x_i;\\mathbf{\\theta}).\n",
    "$$\n",
    "\n",
    "En el caso discreto $L(\\mathbf{\\theta})$ es igual a $\\mathbb{P}_{\\mathbf{\\theta}}\\left( X_1 = x_1 \\cap X_2 = x_2 \\cap \\ldots \\cap X_n = x_n\\right)$\n",
    "\n",
    "**Teorema(Factorización de Neyman)**\n",
    "\n",
    "Una estadística $T$ es suficiente para $\\mathbf{\\theta}$ si y sólo si la siguiente factorización se cumple para toda $x_1,\\ldots,x_n \\in \\mathcal{X}$\n",
    "\n",
    "$$\n",
    "L(\\mathbf{\\theta}) = g(T(x_1,\\ldots,x_n);\\mathbf{\\theta})h(x_1,\\ldots,x_n).\n",
    "$$\n",
    "\n",
    "en donde $g$ y $h$ son funciones no negativas, $h$ no depende de $\\mathbf{\\theta}$ y $g$ únicamente involucra $x_1,\\ldots,x_n$ a través de $T(x_1,\\ldots,x_n)$.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Suponga que $X_1,\\ldots,X_n$ tienen distribución Bernoulli$(p)$ con $p$ desconocida. Entonces\n",
    "\n",
    "$$\n",
    "L(p) = \\Pi_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}=p^{\\sum_{i=1}^{n}x_i}(1-p)^{n-\\sum_{i=1}^{n}x_i}\n",
    "$$\n",
    "\n",
    "en este caso $g(T(x_1,\\ldots,x_n);p) = p^{\\sum_{i=1}^{n}x_i}(1-p)^{n-\\sum_{i=1}^{n}x_i}$ y $h(x_1,\\ldots,x_n)=1$ y $T=\\sum_{i=1}^{n}X_i$ es una estadística suficiente para $p$.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Suponga que $X_1,\\ldots,X_n$ tienen distribución $Poisson(\\lambda)$, con $\\lambda$ desconocida. Entonces\n",
    "\n",
    "$$\n",
    "L(\\lambda) = \\Pi_{i=1}^{n} \\dfrac{e^{-\\lambda} \\lambda^{x_i}  }{x_{i}!} = e^{-n\\lambda}\\lambda^{\\sum_{i=1}^{n}x_i} \\left( \\Pi_{i=1}^{n}x_i! \\right)^{-1}.\n",
    "$$\n",
    "\n",
    "En este caso $g = e^{-n\\lambda}\\lambda^{\\sum_{i=1}^{n}x_i}$, $h = \\left( \\Pi_{i=1}^{n}x_i! \\right)^{-1}$ y $T=\\sum_{i=1}^{n}X_i$.\n",
    "\n",
    "\n",
    "**Teorema**\n",
    "\n",
    "Si $T$ es una estadística suficiente para $\\mathbf{\\theta}$, entonces cualquier estadística $U$ que es una función **uno a uno** de $T$, es suficiente para $\\mathbf{\\theta}$.\n",
    "\n",
    "Así, en los ejemplos anteriores, $\\bar{X}$ es tambíen una estadística suficiente.\n",
    "\n",
    "## Estimación puntual\n",
    "\n",
    "**Definición(Estimador puntual y estimado)**\n",
    "\n",
    "Un estimador puntual de un parámetro desconocido $\\mathbf{\\theta}$ es una función $T(X_1,\\ldots,X_n)$. Una vez observando los valores de la muestra $\\mathbf{X}=\\mathbf{x}$, el valor $t = T(\\mathbf{X})$ es llamado un estimado para $\\mathbf{\\theta}$.\n",
    "\n",
    "**Definición(Estimado y estimador máximo verosímil)**\n",
    "\n",
    "Un **estimado** máximo verosímil de $\\mathbf{\\theta}$, es un valor $\\widehat{\\mathbf{\\theta}(\\mathbf{x})}$ para el cual\n",
    "\n",
    "$$\n",
    "L(\\widehat{ \\mathbf{\\theta}(\\mathbf{x}) } ) = \\max_{\\mathbf{\\theta}}L(\\mathbf{\\theta}).\n",
    "$$\n",
    "\n",
    "El **estimador** máximo verosímil de $\\mathbf{\\theta}$, es la variable aleatoria $\\widehat{\\mathbf{\\theta}(\\mathbf{X})}$.\n",
    "\n",
    "Intuitivamente, un estimador máximo verosímil se interpreta como el valor de $\\mathbf{\\theta}$ que maximiza la \"posibilidad\" de observar un conjunto de datos, $\\mathbf{x}$, en particular.\n",
    "\n",
    "En la práctica, en lugar de maximizar $L(\\mathbf{\\theta})$, se maximiza la función\n",
    "\n",
    "$$\n",
    "\\ln L(\\mathbf{\\theta})\n",
    "$$\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Si $X_1,\\ldots,X_n$ son variables independientes con distribución $N(\\mu, \\sigma^2)$, es posible demostrar que los estimadores máximos verosímiles para $\\mu$ y $\\sigma^2$ son\n",
    "\n",
    "$$\n",
    "\\widehat{\\mu(\\mathbf{X})} = \\bar{X}\n",
    "$$\n",
    "$$\n",
    "\\widehat{ \\sigma^2 (\\mathbf{X}) } = \\dfrac{1}{n} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\n",
    "$$\n",
    "Compruebe lo anterior utilizando\n",
    "```python\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "```\n",
    "junto con una muestra de 100 variables $N(\\mu = 1, \\sigma = 1.5)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def log_like_normal(params, *args):\n",
    "    '''\n",
    "    Función para calcular el logaritmo (natural) de una función\n",
    "    de densidad normal\n",
    "    \n",
    "    ENTRADA\n",
    "    params: Numpy array con dimensión 1 y shape(n,) en donde n es el número\n",
    "    de variables que se quieren optimizar\n",
    "    \n",
    "    *args: Tupla de parámetros fijos necesarios para definir completamente\n",
    "    la función\n",
    "    \n",
    "    SALIDA\n",
    "    Valor de logaritmo (natural) de una función de densidad normal\n",
    "    '''\n",
    "    \n",
    "    datos = args[0] #Valores de cada variable en X_i\n",
    "    mu = params[0]\n",
    "    sig = params[1]\n",
    "    \n",
    "    return -np.sum( norm.logpdf(x = datos, loc = mu, scale = sig) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 176.10512636240364\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-3.41060513e-05, -6.53699317e-05])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 36\n",
      "      nit: 9\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1.33493571, 1.40791837])\n"
     ]
    }
   ],
   "source": [
    "#Genera la una muestra\n",
    "np.random.seed(54321)\n",
    "x = norm.rvs(loc = 1, scale = 1.5, size = 100)\n",
    "\n",
    "#Solución inicial\n",
    "x0 = np.array([0,1])\n",
    "\n",
    "#Cotas\n",
    "#mu puede tomar cualquier valor en [-np.inf, np.inf]\n",
    "#sigma cualquier valor en [0, np.inf], el 0.00001 es para evitar\n",
    "cotas = Bounds(lb = [-np.inf, 0], ub = [np.inf, np.inf])\n",
    "\n",
    "#Obtiene la solución\n",
    "solucion = minimize(log_like_normal,x0 = x0,bounds=cotas, method='L-BFGS-B',args=(x))\n",
    "print(solucion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La solución analítica para mu es  1.334936\n",
      "La solución aproximada para mu es  1.334936\n",
      "La solución analítica para sigma^2 es  1.982236\n",
      "La solución aproximada para sigma^2 es  1.982234\n"
     ]
    }
   ],
   "source": [
    "print(\"La solución analítica para mu es \", np.round(x.mean(), 6))\n",
    "print(\"La solución aproximada para mu es \", np.round(solucion.x[0], 6))\n",
    "print(\"La solución analítica para sigma^2 es \", np.round(x.var(ddof = 0), 6))\n",
    "print(\"La solución aproximada para sigma^2 es \", np.round(solucion.x[1]**2, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teorema(Invarianza de estimadores máximo verosímiles)**\n",
    "\n",
    "Suponga que $\\widehat{ \\mathbf{\\theta} }$ es un estimador máximo verosímil de $\\mathbf{\\theta}$. Sea $g(.)$ una función (no necesariamente uno a uno). Entonces, un estimador máximo verosímil de $g(\\mathbf{\\theta})$ es $g(\\widehat{\\mathbf{\\theta}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La solución analítica para sigma es  1.407919\n",
      "La solución aproximada para sigma es  1.407918\n"
     ]
    }
   ],
   "source": [
    "print(\"La solución analítica para sigma es \", np.round(x.std(ddof = 0), 6))\n",
    "print(\"La solución aproximada para sigma es \", np.round(solucion.x[1], 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición(Estimador insesgado y sesgo)**\n",
    "\n",
    "Una estadística $T$ es un estimador insesgado del parámetro $\\tau(\\mathbf{\\theta})$ si y sólo si para todo $\\mathbf{\\theta}$\n",
    "$$\n",
    "E_{\\mathbf{\\theta}}[T] = \\tau(\\mathbf{\\theta}) \n",
    "$$\n",
    "El sesgo de $T$ se define como\n",
    "$$\n",
    "Sesgo_{\\mathbf{\\theta}}(T) = E_{\\mathbf{\\theta}}[T] - \\tau(\\mathbf{\\theta})\n",
    "$$\n",
    "\n",
    "**Definición(Error cuadrático medio)**\n",
    "\n",
    "El error cuadrático medio de un estimador $T$ para un parámetro $\\tau(\\mathbf{\\theta})$ se define como \n",
    "$$\n",
    "E_{\\mathbf{\\theta}}\\left[ (T - \\tau(\\mathbf{\\theta}))^2 \\right]\n",
    "$$\n",
    "\n",
    "**Teorema(Descomposición del error cuadrático medio)**\n",
    "\n",
    "$$\n",
    "E_{\\mathbf{\\theta}}\\left[ (T- \\tau(\\mathbf{\\theta}))^2 \\right] = Var(T) + Sesgo^2(T)\n",
    "$$\n",
    "\n",
    "**Teorema(Rao-Blackwell)**\n",
    "\n",
    "Suponga que $T$ es un estimador insesgado de una función con valor real $\\tau(\\theta)$. Sea $U$ una estadística suficiente para $\\theta$ y defina $g(u) = E[T|U=u]$. Entonces\n",
    "\n",
    "1. $W = g(U)$ es un estimador insesgado de $\\tau(\\theta)$.\n",
    "\n",
    "2. $Var(W) \\leq Var(T)$ para todo $\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización de la log versosimilitud en el caso poisson\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "def log_lik_poisson(params, *args):\n",
    "    datos = args[0]\n",
    "    lam = params[0]\n",
    "    \n",
    "    return -np.sum(poisson.logpmf(k = datos, mu = lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(54321)\n",
    "simulados = poisson.rvs(size = 100, mu = 2.5)\n",
    "\n",
    "#Punto inicial\n",
    "x0 = np.array([0.5])\n",
    "cotas = Bounds(lb = [0], ub = [np.inf])\n",
    "\n",
    "solucion = minimize(log_lik_poisson, x0 = x0, args = (simulados),\n",
    "                   method = 'L-BFGS-B', bounds = cotas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 181.648880546196\n",
       " hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([5.68434189e-06])\n",
       "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
       "     nfev: 18\n",
       "      nit: 8\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([2.46000006])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulados.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
